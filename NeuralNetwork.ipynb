{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "class NeuralNetworkInterpolator():\n",
    "    \"\"\"cool class\"\"\"\n",
    "    _questions = \"\"\"\n",
    "     inputsize = 3 :: int\n",
    "     outputsize = 3 :: int\n",
    "     hiddenlayer_sizes = [50] :: list\n",
    "     learningrate = 1e-4:: float\n",
    "     epochs = 30000 :: int\n",
    "     epochstep = 50 :: int\n",
    "     loggingfile = logfile.txt :: str\n",
    "     plottingfile = plottingfile.txt :: str\n",
    "     optimizer = adam :: str\n",
    "     normalizer = tanh :: str\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, db, properties, logger, inputsize, outputsize, energy_only=True, savefile='', inverse=False,\n",
    "                 hiddenlayer_sizes=[50], learning_rate=1e-3, epochs=300,epochsteps=50, optimizing=False,loggingfile=\"\",printingfile=\"\",optimizer=\"adam\", normalizer=\"tanh\"):\n",
    "        self.db = db\n",
    "        self.vdb = vdb\n",
    "        \n",
    "        \n",
    "        self.hiddenlayer_sizes = hiddenlayer_sizes\n",
    "        self.dataset = NNDataset(torch.tensor(dbcoord),torch.tensor(np.copy(db['energy'])-np.amin(np.copy(db['energy']))))\n",
    "        self.validationset = NNDataset(torch.tensor(vdbcoord),torch.tensor(np.copy(vdb['energy'])-np.amin(np.copy(db['energy']))))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=256,shuffle= 'True')\n",
    "        print(self.dataset.energyCurves)\n",
    "        self.N = 256\n",
    "        \n",
    "        #self.dataset.output_shape()\n",
    "        self.epochs = epochs\n",
    "        self.optimizing=optimizing\n",
    "        self.epochsteps = epochsteps\n",
    "        if optimizing:\n",
    "            self.loggingfile = loggingfile\n",
    "            self.printingfile = printingfile\n",
    "        # Use the nn package to define our model and loss function.\n",
    "        network = NeuralNet(inputsize, hiddenlayer_sizes, outputsize, optimizer, normalizer)\n",
    "        self.model = network.hidden\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.model.double()\n",
    "    # Use the optim package to define an Optimizer that will update the weights of\n",
    "    # the model for us. Here we will use Adam; the optim package contains many other\n",
    "    # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "    # optimizer which Tensors it should update.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "        super().__init__(db, properties, logger, energy_only, savefile, inverse=inverse)\n",
    "        # N is batch size; D_in is input dimension;\n",
    "        # H is hidden dimension; D_out is output dimension.\n",
    "        # create your dataset\n",
    "        # create your dataloade\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_interpolators(self, db, properties):\n",
    "        print(\"test\")\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.model, filename)\n",
    "\n",
    "    def loadweights(self, filename):\n",
    "        self.model = torch.load(filename)\n",
    "\n",
    "    def get_interpolators_from_file(self, filename, properties):\n",
    "        \"\"\"Properties contains a tuple of [energy,gradient] \"\"\"\n",
    "        return {prop_name: self.db[prop_name].shape[1:] for prop_name in properties}\n",
    "\n",
    "\n",
    "    def get(self, request):\n",
    "        \"\"\"Gives object with coordinates and desired properties\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _train(self):\n",
    "        for t in range(self.epochs):\n",
    "\n",
    "            for index, data in enumerate(self.dataloader,0):\n",
    "                local_batch, local_labels = data\n",
    "                #print(local_batch)\n",
    "                # Forward pass: compute predicted y by passing x to the model.\n",
    "                y_pred = self.model(local_batch)\n",
    "\n",
    "                # Compute and print loss.\n",
    "                #print(y_pred)\n",
    "                #print(local_labels)\n",
    "                loss = self.loss_fn(y_pred, local_labels)\n",
    "                #print(loss)\n",
    "\n",
    "                # Before the backward pass, use the optimizer object to zero all of the\n",
    "                # gradients for the variables it will update (which are the learnable\n",
    "                # weights of the model). This is because by default, gradients are\n",
    "                # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "                # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "                self.optimizer.zero_grad()\n",
    "                # Backward pass: compute gradient of the loss with respect to model\n",
    "                # parameters\n",
    "                loss.backward()\n",
    "\n",
    "                # Calling the step function on an Optimizer makes an update to its\n",
    "                # parameters\n",
    "                self.optimizer.step()\n",
    "                \n",
    "\n",
    "            if t % self.epochsteps == 0:\n",
    "                \n",
    "                lossprint = np.sqrt(self.validate_model(t)/len(self.validationset))\n",
    "                print(f\"{lossprint} {t} {self.hiddenlayer_sizes}\")\n",
    "                self.printingfile.append((self.hiddenlayer_sizes,t,lossprint))\n",
    "                with open('printoutNormGroundBGrid.txt', 'wb') as f:\n",
    "                    pickle.dump(self.printingfile,f)\n",
    "                file = open(self.loggingfile,\"a\")\n",
    "                file.write(f\"Loss = {lossprint} \\n Epochs = {self.epochs} \\n Hiddenlayers = {self.hiddenlayer_sizes} \\n Learningrate = {self.learning_rate} \\n Validation_percent = {self.validation_percent} \\n Batchsize = {self.N} \\n \\n\")\n",
    "                file.close()\n",
    "\n",
    "    \n",
    "                if self.optimizing == \"Seperate\":\n",
    "                    loss = np.sqrt(self.validate_model()/len(self.validationset))\n",
    "                    self.printingfile.append((self.hiddenlayer_sizes,self.epochs,loss))\n",
    "                    with open('printoutsixlayer.txt', 'wb') as f:\n",
    "                        pickle.dump(self.printingfile,f)\n",
    "                    file = open(self.loggingfile,\"a\")\n",
    "                    file.write(f\"Loss = {loss} \\n Epochs = {t} \\n Hiddenlayers = {self.hiddenlayer_sizes} \\n Learningrate = {self.learning_rate} \\n Validation_percent = {self.validation_percent} \\n Batchsize = {self.N} \\n \\n\")\n",
    "                    file.close()\n",
    "\n",
    "        print(\"Done Training\")\n",
    "    def validate_model(self,n):\n",
    "        model_predictions = []\n",
    "        testpoint_positions = []\n",
    "        losssquared = 0\n",
    "        for local_batch, local_labels in self.validationset:\n",
    "                # Forward pass: compute predicted y by passing x to the model.\n",
    "                y_pred = self.model(torch.flatten(local_batch))\n",
    "                #print(y_pred)\n",
    "                model_predictions.append(y_pred.tolist())\n",
    "                # Compute and print loss\n",
    "                testpoint_positions.append(local_batch.tolist())\n",
    "                loss = self.loss_fn(y_pred, local_labels)\n",
    "                losssquared += loss.item()\n",
    "        #print(model_predictions)\n",
    "        if np.sqrt(losssquared /len(self.validationset))< 0.001 :\n",
    "            plt.plot(self.vdb['energy']-np.amin(self.vdb['energy']))\n",
    "            plt.plot(model_predictions)\n",
    "            \n",
    "            plt.savefig(f\"graph{n}{self.hiddenlayer_sizes}sixlayer.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        return losssquared\n",
    "\n",
    "class NNDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Molecule Data set\"\"\"\n",
    "\n",
    "    def __init__(self, coordinates, energyCurves):\n",
    "        self.coordinates = coordinates\n",
    "        self.energyCurves = energyCurves\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        coordinate = self.coordinates[index]\n",
    "        curve = self.energyCurves[index]\n",
    "\n",
    "        return coordinate, curve\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coordinates)\n",
    "        \n",
    "    def input_shape(self):\n",
    "        return list(self.coordinates[0].size())[0] *3\n",
    "    def output_shape(self):\n",
    "        return list(self.energyCurves[0].size())[0]\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out,optimizer=\"adam\", normalizer=\"tanh\"):\n",
    "        H = [D_in] + H + []\n",
    "        self.hidden = torch.nn.ModuleList()\n",
    "        if normalizer == \"tanh\":\n",
    "            for k in range(len(H)-2):\n",
    "                self.hidden.append(torch.nn.Linear(H[k], H[k+1]))\n",
    "                self.hidden.append(torch.nn.Tanh())\n",
    "            self.hidden.append(torch.nn.Linear(H[-2],H[-1]))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterEnv",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
